# src/scouts/social_sentiment_scout.py
"""
ç¤¾äº¤æƒ…ç»ªä¾¦å¯Ÿå…µ - æ ¹æ®PDFå»ºè®®å®žçŽ°
ç›‘æŽ§Twitterã€Redditç­‰å¹³å°çš„å¸‚åœºæƒ…ç»ª
"""
import asyncio
import aiohttp
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
import numpy as np
import re
from collections import defaultdict
import logging

# æƒ…ç»ªåˆ†æžåº“
from textblob import TextBlob
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer

from .base_scout import BaseScout, OpportunitySignal

logger = logging.getLogger(__name__)

class SocialSentimentScout(BaseScout):
    """
    ç¤¾äº¤æƒ…ç»ªæ‰«æå™¨ - å®žçŽ°PDFä¸­å»ºè®®çš„æƒ…ç»ªåˆ†æžåŠŸèƒ½
    """
    
    async def _initialize(self):
        """åˆå§‹åŒ–ç¤¾äº¤æƒ…ç»ªScout"""
        # ä¸‹è½½NLTKæ•°æ®
        try:
            nltk.download('vader_lexicon', quiet=True)
            self.sia = SentimentIntensityAnalyzer()
        except:
            logger.warning("VADERæƒ…ç»ªåˆ†æžå™¨åˆå§‹åŒ–å¤±è´¥")
            self.sia = None
        
        # é…ç½®å‚æ•°
        self.monitored_tokens = self.config.get('monitored_tokens', [
            'BTC', 'ETH', 'SOL', 'AVAX', 'MATIC', 'LINK', 'UNI', 'AAVE'
        ])
        
        self.platforms = self.config.get('platforms', ['twitter', 'reddit'])
        self.min_mentions_threshold = self.config.get('min_mentions', 10)
        self.sentiment_change_threshold = self.config.get('sentiment_threshold', 0.3)
        
        # åŽ†å²æ•°æ®ç¼“å­˜
        self.sentiment_history = defaultdict(lambda: defaultdict(list))
        self.mention_history = defaultdict(lambda: defaultdict(list))
        
        # å…³é”®è¯å’Œæƒé‡
        self.bullish_keywords = {
            'moon': 2.0, 'bullish': 1.5, 'pump': 1.5, 'breakout': 1.8,
            'buy': 1.2, 'long': 1.3, 'rocket': 2.0, 'gem': 1.5,
            'undervalued': 1.7, 'accumulate': 1.5, 'hodl': 1.3
        }
        
        self.bearish_keywords = {
            'dump': -2.0, 'bearish': -1.5, 'sell': -1.2, 'short': -1.3,
            'crash': -2.0, 'scam': -2.5, 'rug': -2.5, 'overvalued': -1.7,
            'bubble': -1.8, 'correction': -1.5
        }
        
        # å½±å“åŠ›æƒé‡ï¼ˆç²‰ä¸æ•°èŒƒå›´ï¼‰
        self.influence_weights = {
            'micro': (100, 1000, 1.0),      # 100-1k followers
            'small': (1000, 10000, 1.5),    # 1k-10k
            'medium': (10000, 100000, 2.0), # 10k-100k
            'large': (100000, 1000000, 3.0), # 100k-1M
            'mega': (1000000, float('inf'), 5.0)  # 1M+
        }
    
    async def scan(self) -> List[OpportunitySignal]:
        """æ‰§è¡Œç¤¾äº¤æƒ…ç»ªæ‰«æ"""
        opportunities = []
        
        tasks = []
        for token in self.monitored_tokens:
            tasks.append(self._analyze_token_sentiment(token))
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        for result in results:
            if isinstance(result, list):
                opportunities.extend(result)
            elif isinstance(result, Exception):
                logger.error(f"ç¤¾äº¤æƒ…ç»ªæ‰«æé”™è¯¯: {result}")
        
        return opportunities
    
    async def _analyze_token_sentiment(self, token: str) -> List[OpportunitySignal]:
        """åˆ†æžå•ä¸ªä»£å¸çš„ç¤¾äº¤æƒ…ç»ª"""
        opportunities = []
        
        # æ”¶é›†å„å¹³å°æ•°æ®
        platform_data = {}
        
        if 'twitter' in self.platforms:
            twitter_data = await self._fetch_twitter_data(token)
            if twitter_data:
                platform_data['twitter'] = twitter_data
        
        if 'reddit' in self.platforms:
            reddit_data = await self._fetch_reddit_data(token)
            if reddit_data:
                platform_data['reddit'] = reddit_data
        
        # èšåˆåˆ†æž
        if platform_data:
            analysis = self._aggregate_sentiment_analysis(token, platform_data)
            
            # æ£€æµ‹æƒ…ç»ªçªå˜
            if analysis['sentiment_change'] and abs(analysis['sentiment_delta']) > self.sentiment_change_threshold:
                opportunity = self.create_opportunity(
                    signal_type='sentiment_shift',
                    symbol=f"{token}/USDT",
                    confidence=min(abs(analysis['sentiment_delta']) * 2, 0.95),
                    data={
                        'token': token,
                        'current_sentiment': analysis['current_sentiment'],
                        'sentiment_delta': analysis['sentiment_delta'],
                        'mention_count': analysis['total_mentions'],
                        'mention_delta_pct': analysis['mention_change_pct'],
                        'platforms': list(platform_data.keys()),
                        'influencer_mentions': analysis['influencer_mentions'],
                        'trending_score': analysis['trending_score'],
                        'bullish_ratio': analysis['bullish_ratio'],
                        'key_narratives': analysis['key_narratives']
                    },
                    expires_in_minutes=30
                )
                opportunities.append(opportunity)
                
                logger.info(f"ðŸ˜Š æƒ…ç»ªè½¬å˜: {token} - "
                           f"å˜åŒ–: {analysis['sentiment_delta']:+.2f}, "
                           f"æåŠ: {analysis['total_mentions']}")
            
            # æ£€æµ‹æåŠé‡æ¿€å¢ž
            if analysis['mention_spike'] and analysis['mention_change_pct'] > 200:
                opportunity = self.create_opportunity(
                    signal_type='social_volume_spike',
                    symbol=f"{token}/USDT",
                    confidence=min(analysis['mention_change_pct'] / 500, 0.9),
                    data={
                        'token': token,
                        'mention_count': analysis['total_mentions'],
                        'mention_change_pct': analysis['mention_change_pct'],
                        'sentiment': analysis['current_sentiment'],
                        'platforms': list(platform_data.keys()),
                        'viral_posts': analysis.get('viral_posts', [])
                    },
                    expires_in_minutes=60
                )
                opportunities.append(opportunity)
                
                logger.info(f"ðŸ“¢ ç¤¾äº¤é‡æ¿€å¢ž: {token} - "
                           f"å¢žé•¿: {analysis['mention_change_pct']:.0f}%")
        
        return opportunities
    
    async def _fetch_twitter_data(self, token: str) -> Optional[Dict]:
        """èŽ·å–Twitteræ•°æ®ï¼ˆä½¿ç”¨å¼€æºå·¥å…·ï¼‰"""
        try:
            # è¿™é‡Œåº”è¯¥ä½¿ç”¨ntscraperæˆ–å…¶ä»–å¼€æºå·¥å…·
            # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬æ¨¡æ‹Ÿæ•°æ®ç»“æž„
            
            posts = []
            
            # æ¨¡æ‹ŸAPIè°ƒç”¨
            # å®žé™…å®žçŽ°æ—¶ä½¿ç”¨ï¼š
            # from ntscraper import Nitter
            # scraper = Nitter()
            # posts = await scraper.get_tweets(f"${token} OR #{token}crypto")
            
            # åˆ†æžæƒ…ç»ª
            sentiments = []
            total_followers = 0
            influencer_posts = []
            
            for post in posts:
                # åˆ†æžå•æ¡æŽ¨æ–‡
                sentiment = self._analyze_text_sentiment(post.get('text', ''))
                sentiments.append(sentiment)
                
                followers = post.get('user', {}).get('followers_count', 0)
                total_followers += followers
                
                # è¯†åˆ«å½±å“åŠ›ç”¨æˆ·
                if followers > 10000:
                    influencer_posts.append({
                        'user': post['user']['screen_name'],
                        'followers': followers,
                        'text': post['text'][:200],
                        'sentiment': sentiment
                    })
            
            if sentiments:
                return {
                    'posts_count': len(posts),
                    'avg_sentiment': np.mean(sentiments),
                    'sentiment_std': np.std(sentiments),
                    'total_reach': total_followers,
                    'influencer_posts': influencer_posts[:5],  # Top 5
                    'positive_ratio': sum(1 for s in sentiments if s > 0.1) / len(sentiments),
                    'negative_ratio': sum(1 for s in sentiments if s < -0.1) / len(sentiments)
                }
            
        except Exception as e:
            logger.error(f"èŽ·å–Twitteræ•°æ®å¤±è´¥ {token}: {e}")
        
        return None
    
    async def _fetch_reddit_data(self, token: str) -> Optional[Dict]:
        """èŽ·å–Redditæ•°æ®"""
        try:
            # ä½¿ç”¨PRAWåº“
            # import praw
            # reddit = praw.Reddit(client_id='...', client_secret='...', user_agent='...')
            
            subreddits = ['cryptocurrency', 'CryptoMoonShots', 'altcoin', token.lower()]
            
            posts_data = []
            comments_data = []
            
            # æ¨¡æ‹Ÿæ•°æ®èŽ·å–
            # å®žé™…å®žçŽ°ï¼š
            # for subreddit_name in subreddits:
            #     subreddit = reddit.subreddit(subreddit_name)
            #     for post in subreddit.search(token, time_filter='day', limit=50):
            #         posts_data.append({...})
            
            # åˆ†æžæƒ…ç»ª
            post_sentiments = [self._analyze_text_sentiment(p.get('title', '') + ' ' + p.get('selftext', '')) 
                              for p in posts_data]
            comment_sentiments = [self._analyze_text_sentiment(c.get('body', '')) 
                                 for c in comments_data]
            
            all_sentiments = post_sentiments + comment_sentiments
            
            if all_sentiments:
                return {
                    'posts_count': len(posts_data),
                    'comments_count': len(comments_data),
                    'avg_sentiment': np.mean(all_sentiments),
                    'sentiment_std': np.std(all_sentiments),
                    'upvote_ratio': np.mean([p.get('upvote_ratio', 0.5) for p in posts_data]),
                    'total_score': sum(p.get('score', 0) for p in posts_data),
                    'hot_posts': sorted(posts_data, key=lambda x: x.get('score', 0), reverse=True)[:3]
                }
            
        except Exception as e:
            logger.error(f"èŽ·å–Redditæ•°æ®å¤±è´¥ {token}: {e}")
        
        return None
    
    def _analyze_text_sentiment(self, text: str) -> float:
        """åˆ†æžæ–‡æœ¬æƒ…ç»ª"""
        if not text:
            return 0.0
        
        # æ¸…ç†æ–‡æœ¬
        text = re.sub(r'http\S+', '', text)  # ç§»é™¤URLs
        text = re.sub(r'@\w+', '', text)     # ç§»é™¤æåŠ
        text = re.sub(r'#(\w+)', r'\1', text) # ç§»é™¤#ä½†ä¿ç•™æ ‡ç­¾æ–‡æœ¬
        
        # åŸºç¡€æƒ…ç»ªåˆ†æž
        sentiment_scores = []
        
        # 1. TextBlobåˆ†æž
        try:
            blob = TextBlob(text)
            sentiment_scores.append(blob.sentiment.polarity)
        except:
            pass
        
        # 2. VADERåˆ†æž
        if self.sia:
            scores = self.sia.polarity_scores(text)
            sentiment_scores.append(scores['compound'])
        
        # 3. å…³é”®è¯åˆ†æž
        keyword_score = 0
        text_lower = text.lower()
        
        for word, weight in self.bullish_keywords.items():
            if word in text_lower:
                keyword_score += weight
        
        for word, weight in self.bearish_keywords.items():
            if word in text_lower:
                keyword_score += weight
        
        # å½’ä¸€åŒ–å…³é”®è¯åˆ†æ•°
        if keyword_score != 0:
            keyword_score = np.tanh(keyword_score / 5)  # é™åˆ¶åœ¨[-1, 1]
            sentiment_scores.append(keyword_score)
        
        # ç»¼åˆå¾—åˆ†
        if sentiment_scores:
            return np.mean(sentiment_scores)
        
        return 0.0
    
    def _aggregate_sentiment_analysis(self, token: str, platform_data: Dict) -> Dict:
        """èšåˆå¤šå¹³å°æƒ…ç»ªåˆ†æž"""
        current_time = datetime.now()
        
        # è®¡ç®—ç»¼åˆæŒ‡æ ‡
        total_mentions = sum(
            data.get('posts_count', 0) + data.get('comments_count', 0)
            for data in platform_data.values()
        )
        
        # åŠ æƒå¹³å‡æƒ…ç»ªï¼ˆæ ¹æ®å¹³å°æƒé‡ï¼‰
        platform_weights = {'twitter': 1.5, 'reddit': 1.0, 'telegram': 0.8}
        
        weighted_sentiment = 0
        total_weight = 0
        
        for platform, data in platform_data.items():
            weight = platform_weights.get(platform, 1.0)
            sentiment = data.get('avg_sentiment', 0)
            weighted_sentiment += sentiment * weight
            total_weight += weight
        
        current_sentiment = weighted_sentiment / total_weight if total_weight > 0 else 0
        
        # èŽ·å–åŽ†å²æ•°æ®
        history = self.sentiment_history[token]['aggregate']
        history.append({
            'time': current_time,
            'sentiment': current_sentiment,
            'mentions': total_mentions
        })
        
        # ä¿ç•™æœ€è¿‘24å°æ—¶æ•°æ®
        cutoff = current_time - timedelta(hours=24)
        history = [h for h in history if h['time'] > cutoff]
        self.sentiment_history[token]['aggregate'] = history
        
        # è®¡ç®—å˜åŒ–
        sentiment_change = False
        sentiment_delta = 0
        mention_spike = False
        mention_change_pct = 0
        
        if len(history) >= 10:
            # æ¯”è¾ƒæœ€è¿‘1å°æ—¶vså‰3å°æ—¶
            recent = [h for h in history if h['time'] > current_time - timedelta(hours=1)]
            previous = [h for h in history if current_time - timedelta(hours=4) < h['time'] <= current_time - timedelta(hours=1)]
            
            if recent and previous:
                recent_sentiment = np.mean([h['sentiment'] for h in recent])
                previous_sentiment = np.mean([h['sentiment'] for h in previous])
                sentiment_delta = recent_sentiment - previous_sentiment
                sentiment_change = abs(sentiment_delta) > self.sentiment_change_threshold
                
                recent_mentions = sum(h['mentions'] for h in recent)
                previous_mentions = sum(h['mentions'] for h in previous) / 3  # å¹³å‡æ¯å°æ—¶
                
                if previous_mentions > 0:
                    mention_change_pct = ((recent_mentions - previous_mentions) / previous_mentions) * 100
                    mention_spike = mention_change_pct > 100
        
        # è®¡ç®—å…¶ä»–æŒ‡æ ‡
        influencer_mentions = sum(
            len(data.get('influencer_posts', []))
            for data in platform_data.values()
        )
        
        # è¶‹åŠ¿è¯„åˆ†ï¼ˆåŸºäºŽæåŠå¢žé•¿å’Œæƒ…ç»ªï¼‰
        trending_score = min(
            (mention_change_pct / 100) * 0.5 + 
            abs(sentiment_delta) * 2 + 
            (influencer_mentions / 10),
            10.0
        )
        
        # å¤šç©ºæ¯”ä¾‹
        bullish_count = sum(
            data.get('posts_count', 0) * data.get('positive_ratio', 0.5)
            for data in platform_data.values()
        )
        bearish_count = sum(
            data.get('posts_count', 0) * data.get('negative_ratio', 0.5)
            for data in platform_data.values()
        )
        
        bullish_ratio = bullish_count / (bullish_count + bearish_count) if (bullish_count + bearish_count) > 0 else 0.5
        
        # æå–å…³é”®å™äº‹
        key_narratives = []
        for data in platform_data.values():
            if 'hot_posts' in data:
                for post in data['hot_posts'][:2]:
                    key_narratives.append(post.get('title', '')[:100])
            if 'viral_posts' in data:
                for post in data['viral_posts'][:2]:
                    key_narratives.append(post.get('text', '')[:100])
        
        return {
            'current_sentiment': current_sentiment,
            'sentiment_delta': sentiment_delta,
            'sentiment_change': sentiment_change,
            'total_mentions': total_mentions,
            'mention_change_pct': mention_change_pct,
            'mention_spike': mention_spike,
            'influencer_mentions': influencer_mentions,
            'trending_score': trending_score,
            'bullish_ratio': bullish_ratio,
            'key_narratives': key_narratives[:5]
        }


class DeveloperActivityScout(BaseScout):
    """
    å¼€å‘è€…æ´»åŠ¨ä¾¦å¯Ÿå…µ - ç›‘æŽ§GitHubæ´»åŠ¨
    """
    
    async def _initialize(self):
        """åˆå§‹åŒ–å¼€å‘è€…æ´»åŠ¨Scout"""
        self.github_token = self.config.get('github_token')
        self.monitored_repos = self.config.get('monitored_repos', {})
        
        # æ´»åŠ¨åŽ†å²
        self.activity_history = defaultdict(list)
        
        # GitHub APIåŸºç¡€URL
        self.github_api = "https://api.github.com"
        
        # è¯·æ±‚å¤´
        self.headers = {
            'Accept': 'application/vnd.github.v3+json'
        }
        if self.github_token:
            self.headers['Authorization'] = f'token {self.github_token}'
    
    async def scan(self) -> List[OpportunitySignal]:
        """æ‰«æå¼€å‘è€…æ´»åŠ¨"""
        opportunities = []
        
        for token, repo_url in self.monitored_repos.items():
            if 'github.com' in repo_url:
                repo_path = repo_url.split('github.com/')[-1].strip('/')
                activity = await self._analyze_repo_activity(token, repo_path)
                
                if activity:
                    opportunities.extend(activity)
        
        return opportunities
    
    async def _analyze_repo_activity(self, token: str, repo_path: str) -> List[OpportunitySignal]:
        """åˆ†æžä»“åº“æ´»åŠ¨"""
        opportunities = []
        
        try:
            # èŽ·å–ä»“åº“ç»Ÿè®¡
            repo_stats = await self._fetch_repo_stats(repo_path)
            commit_activity = await self._fetch_commit_activity(repo_path)
            pr_activity = await self._fetch_pr_activity(repo_path)
            
            if repo_stats:
                # è®¡ç®—æ´»åŠ¨è¯„åˆ†
                activity_score = self._calculate_activity_score(
                    repo_stats, commit_activity, pr_activity
                )
                
                # æ£€æµ‹æ´»åŠ¨å¼‚å¸¸
                if activity_score > 7:  # é«˜æ´»è·ƒåº¦
                    opportunity = self.create_opportunity(
                        signal_type='high_dev_activity',
                        symbol=f"{token}/USDT",
                        confidence=min(activity_score / 10, 0.9),
                        data={
                            'token': token,
                            'repo': repo_path,
                            'activity_score': activity_score,
                            'commits_30d': commit_activity.get('commits_30d', 0),
                            'active_contributors': commit_activity.get('contributors', 0),
                            'open_prs': pr_activity.get('open_count', 0),
                            'stars': repo_stats.get('stargazers_count', 0),
                            'forks': repo_stats.get('forks_count', 0)
                        }
                    )
                    opportunities.append(opportunity)
                    
                    logger.info(f"ðŸ‘¨â€ðŸ’» é«˜å¼€å‘æ´»åŠ¨: {token} - è¯„åˆ†: {activity_score:.1f}")
        
        except Exception as e:
            logger.error(f"åˆ†æžä»“åº“æ´»åŠ¨å¤±è´¥ {repo_path}: {e}")
        
        return opportunities
    
    async def _fetch_repo_stats(self, repo_path: str) -> Optional[Dict]:
        """èŽ·å–ä»“åº“åŸºç¡€ç»Ÿè®¡"""
        try:
            url = f"{self.github_api}/repos/{repo_path}"
            async with self.session.get(url, headers=self.headers) as response:
                if response.status == 200:
                    return await response.json()
        except Exception as e:
            logger.error(f"èŽ·å–ä»“åº“ç»Ÿè®¡å¤±è´¥: {e}")
        return None
    
    async def _fetch_commit_activity(self, repo_path: str) -> Dict:
        """èŽ·å–æäº¤æ´»åŠ¨"""
        try:
            url = f"{self.github_api}/repos/{repo_path}/stats/commit_activity"
            async with self.session.get(url, headers=self.headers) as response:
                if response.status == 200:
                    data = await response.json()
                    # è®¡ç®—30å¤©æäº¤æ•°
                    commits_30d = sum(week['total'] for week in data[-4:])
                    return {'commits_30d': commits_30d}
        except Exception as e:
            logger.error(f"èŽ·å–æäº¤æ´»åŠ¨å¤±è´¥: {e}")
        return {}
    
    async def _fetch_pr_activity(self, repo_path: str) -> Dict:
        """èŽ·å–PRæ´»åŠ¨"""
        try:
            url = f"{self.github_api}/repos/{repo_path}/pulls"
            params = {'state': 'open', 'per_page': 100}
            async with self.session.get(url, headers=self.headers, params=params) as response:
                if response.status == 200:
                    prs = await response.json()
                    return {'open_count': len(prs)}
        except Exception as e:
            logger.error(f"èŽ·å–PRæ´»åŠ¨å¤±è´¥: {e}")
        return {}
    
    def _calculate_activity_score(self, repo_stats: Dict, commit_activity: Dict, pr_activity: Dict) -> float:
        """è®¡ç®—æ´»åŠ¨è¯„åˆ†"""
        score = 0
        
        # åŸºäºŽæäº¤é¢‘çŽ‡
        commits = commit_activity.get('commits_30d', 0)
        if commits > 100:
            score += 3
        elif commits > 50:
            score += 2
        elif commits > 20:
            score += 1
        
        # åŸºäºŽæ˜Ÿæ ‡æ•°
        stars = repo_stats.get('stargazers_count', 0)
        if stars > 5000:
            score += 2
        elif stars > 1000:
            score += 1
        
        # åŸºäºŽForkæ•°
        forks = repo_stats.get('forks_count', 0)
        if forks > 500:
            score += 2
        elif forks > 100:
            score += 1
        
        # åŸºäºŽPRæ´»åŠ¨
        open_prs = pr_activity.get('open_count', 0)
        if open_prs > 20:
            score += 2
        elif open_prs > 10:
            score += 1
        
        return score