# src/analysis/ml_predictor_enhanced.py
"""
å¢å¼ºç‰ˆæœºå™¨å­¦ä¹ é¢„æµ‹å™¨ - æ ¹æ®PDFå»ºè®®é›†æˆSHAPå¯è§£é‡Šæ€§
å®ç°LSTM+XGBoostæ··åˆæ¨¡å‹
"""
import asyncio
import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Tuple, Any
from datetime import datetime, timedelta
import joblib
from pathlib import Path
import logging

# æœºå™¨å­¦ä¹ åº“
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, TimeSeriesSplit
from sklearn.metrics import accuracy_score, precision_recall_curve, roc_auc_score
import xgboost as xgb

# æ·±åº¦å­¦ä¹ 
import tensorflow as tf
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, concatenate
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

# SHAPå¯è§£é‡Šæ€§
import shap

logger = logging.getLogger(__name__)

class EnhancedMLPredictor:
    """
    å¢å¼ºç‰ˆMLé¢„æµ‹å™¨ - å®ç°PDFä¸­å»ºè®®çš„LSTM+XGBoostæ··åˆæ¶æ„
    """
    
    def __init__(self, config):
        self.config = config
        self.models = {}
        self.scalers = {}
        self.shap_explainers = {}
        
        # æ¨¡å‹è·¯å¾„
        self.model_path = Path(config.get('ML_MODEL_PATH', 'ml_models'))
        self.model_path.mkdir(exist_ok=True)
        
        # æ¨¡å‹å‚æ•°
        self.sequence_length = 60  # LSTMçš„æ—¶é—´åºåˆ—é•¿åº¦
        self.prediction_horizon = 5  # é¢„æµ‹æœªæ¥5åˆ†é’Ÿ
        
        # ç‰¹å¾é…ç½®
        self.time_series_features = [
            'price', 'volume', 'bid', 'ask', 'spread',
            'rsi', 'macd', 'bollinger_upper', 'bollinger_lower'
        ]
        
        self.cross_sectional_features = [
            'sentiment_score', 'mention_count', 'dev_activity_score',
            'whale_movement_count', 'exchange_inflow', 'exchange_outflow',
            'gas_price', 'market_cap_rank', 'volume_rank'
        ]
        
        self.feature_importance_history = []
    
    async def initialize(self):
        """åˆå§‹åŒ–é¢„æµ‹å™¨"""
        logger.info("åˆå§‹åŒ–å¢å¼ºç‰ˆMLé¢„æµ‹å™¨...")
        
        # åŠ è½½å·²æœ‰æ¨¡å‹
        await self._load_models()
        
        # åˆå§‹åŒ–SHAPè§£é‡Šå™¨
        self._initialize_shap_explainers()
        
        # å¯åŠ¨å®šæœŸé‡è®­ç»ƒ
        asyncio.create_task(self._periodic_retrain())
        
        logger.info("âœ… MLé¢„æµ‹å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _build_lstm_model(self, sequence_shape: Tuple) -> Model:
        """
        æ„å»ºLSTMæ¨¡å‹ç”¨äºæ—¶é—´åºåˆ—ç‰¹å¾æå–
        """
        inputs = Input(shape=sequence_shape)
        
        # LSTMå±‚
        x = LSTM(128, return_sequences=True, dropout=0.2)(inputs)
        x = LSTM(64, return_sequences=True, dropout=0.2)(x)
        x = LSTM(32, dropout=0.2)(x)
        
        # è¾“å‡ºæ—¶åºç‰¹å¾å‘é‡
        time_features = Dense(16, activation='relu', name='time_features')(x)
        
        model = Model(inputs=inputs, outputs=time_features)
        return model
    
    def _build_hybrid_model(
        self, 
        sequence_shape: Tuple,
        cross_sectional_shape: Tuple
    ) -> Model:
        """
        æ„å»ºLSTM+XGBoostæ··åˆæ¨¡å‹
        è¿™é‡Œæˆ‘ä»¬ç”¨ç¥ç»ç½‘ç»œæ¨¡æ‹Ÿæ··åˆæ¶æ„
        """
        # LSTMè¾“å…¥
        sequence_input = Input(shape=sequence_shape, name='sequence_input')
        
        # LSTMå¤„ç†
        lstm_x = LSTM(128, return_sequences=True, dropout=0.2)(sequence_input)
        lstm_x = LSTM(64, dropout=0.2)(lstm_x)
        lstm_features = Dense(32, activation='relu')(lstm_x)
        
        # æ¨ªæˆªé¢ç‰¹å¾è¾“å…¥
        cross_input = Input(shape=cross_sectional_shape, name='cross_input')
        cross_features = Dense(32, activation='relu')(cross_input)
        cross_features = Dropout(0.3)(cross_features)
        
        # åˆå¹¶ç‰¹å¾
        combined = concatenate([lstm_features, cross_features])
        
        # æ·±å±‚å¤„ç†
        x = Dense(64, activation='relu')(combined)
        x = Dropout(0.3)(x)
        x = Dense(32, activation='relu')(x)
        x = Dropout(0.2)(x)
        
        # è¾“å‡ºå±‚
        output = Dense(1, activation='sigmoid', name='opportunity_score')(x)
        
        model = Model(
            inputs=[sequence_input, cross_input],
            outputs=output
        )
        
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='binary_crossentropy',
            metrics=['accuracy', tf.keras.metrics.AUC()]
        )
        
        return model
    
    def _train_xgboost_model(
        self,
        X_train: np.ndarray,
        y_train: np.ndarray,
        X_val: np.ndarray,
        y_val: np.ndarray
    ) -> xgb.XGBClassifier:
        """
        è®­ç»ƒXGBoostæ¨¡å‹
        """
        model = xgb.XGBClassifier(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            objective='binary:logistic',
            use_label_encoder=False,
            eval_metric='auc',
            random_state=42
        )
        
        # è®­ç»ƒæ—¶ä½¿ç”¨æ—©åœ
        model.fit(
            X_train, y_train,
            eval_set=[(X_val, y_val)],
            early_stopping_rounds=10,
            verbose=False
        )
        
        return model
    
    async def predict_opportunity_with_explanation(
        self,
        opportunity: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        é¢„æµ‹æœºä¼šè´¨é‡å¹¶æä¾›SHAPè§£é‡Š
        """
        try:
            # æå–ç‰¹å¾
            features = await self._extract_features(opportunity)
            
            if features is None:
                return {
                    'prediction_score': opportunity.get('confidence', 0.5),
                    'shap_values': {},
                    'feature_importance': {},
                    'explanation': "ç‰¹å¾æå–å¤±è´¥"
                }
            
            # ä½¿ç”¨æ··åˆæ¨¡å‹é¢„æµ‹
            if 'hybrid_model' in self.models:
                # åˆ†ç¦»æ—¶åºå’Œæ¨ªæˆªé¢ç‰¹å¾
                sequence_features = features['sequence']
                cross_features = features['cross_sectional']
                
                # æ ‡å‡†åŒ–
                if 'sequence_scaler' in self.scalers:
                    sequence_features = self.scalers['sequence_scaler'].transform(sequence_features)
                if 'cross_scaler' in self.scalers:
                    cross_features = self.scalers['cross_scaler'].transform(cross_features)
                
                # é¢„æµ‹
                prediction = self.models['hybrid_model'].predict(
                    [sequence_features, cross_features]
                )[0][0]
            else:
                # å›é€€åˆ°ç®€å•æ¨¡å‹
                prediction = opportunity.get('confidence', 0.5)
            
            # ä½¿ç”¨XGBoostç”ŸæˆSHAPå€¼
            shap_values = {}
            feature_importance = {}
            
            if 'xgboost_model' in self.models and 'xgboost_explainer' in self.shap_explainers:
                # åˆå¹¶æ‰€æœ‰ç‰¹å¾ç”¨äºXGBoost
                all_features = np.concatenate([
                    features['sequence'].flatten(),
                    features['cross_sectional']
                ])
                
                # è·å–SHAPå€¼
                shap_values_array = self.shap_explainers['xgboost_explainer'].shap_values(
                    all_features.reshape(1, -1)
                )[0]
                
                # æ˜ å°„åˆ°ç‰¹å¾åç§°
                feature_names = self._get_all_feature_names()
                shap_values = dict(zip(feature_names, shap_values_array))
                
                # è®¡ç®—ç‰¹å¾é‡è¦æ€§ï¼ˆç»å¯¹SHAPå€¼ï¼‰
                feature_importance = {
                    name: abs(value) 
                    for name, value in shap_values.items()
                }
                
                # æ’åºå¹¶è·å–topç‰¹å¾
                top_features = sorted(
                    feature_importance.items(),
                    key=lambda x: x[1],
                    reverse=True
                )[:10]
            
            # ç”Ÿæˆäººç±»å¯è¯»çš„è§£é‡Š
            explanation = self._generate_explanation(
                prediction,
                shap_values,
                opportunity
            )
            
            return {
                'prediction_score': float(prediction),
                'original_confidence': opportunity.get('confidence', 0),
                'shap_values': shap_values,
                'feature_importance': dict(top_features) if 'top_features' in locals() else {},
                'explanation': explanation,
                'model_confidence': self._calculate_model_confidence(prediction)
            }
            
        except Exception as e:
            logger.error(f"é¢„æµ‹å¤±è´¥: {e}", exc_info=True)
            return {
                'prediction_score': opportunity.get('confidence', 0.5),
                'shap_values': {},
                'feature_importance': {},
                'explanation': f"é¢„æµ‹é”™è¯¯: {str(e)}"
            }
    
    def _initialize_shap_explainers(self):
        """åˆå§‹åŒ–SHAPè§£é‡Šå™¨"""
        try:
            if 'xgboost_model' in self.models:
                # åˆ›å»ºèƒŒæ™¯æ•°æ®é›†ï¼ˆç”¨äºSHAPï¼‰
                # è¿™é‡Œåº”è¯¥ä½¿ç”¨çœŸå®çš„è®­ç»ƒæ•°æ®å­é›†
                background_data = np.random.randn(100, 100)  # ç¤ºä¾‹
                
                self.shap_explainers['xgboost_explainer'] = shap.Explainer(
                    self.models['xgboost_model'],
                    background_data
                )
                logger.info("âœ… SHAPè§£é‡Šå™¨åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–SHAPå¤±è´¥: {e}")
    
    def _generate_explanation(
        self,
        prediction: float,
        shap_values: Dict[str, float],
        opportunity: Dict
    ) -> str:
        """
        ç”Ÿæˆäººç±»å¯è¯»çš„é¢„æµ‹è§£é‡Š
        """
        explanation_parts = []
        
        # é¢„æµ‹å¼ºåº¦
        if prediction > 0.8:
            explanation_parts.append("ğŸŸ¢ å¼ºçƒˆæ¨èï¼šæ¨¡å‹é«˜åº¦ç¡®ä¿¡è¿™æ˜¯ä¸€ä¸ªä¼˜è´¨æœºä¼š")
        elif prediction > 0.6:
            explanation_parts.append("ğŸŸ¡ è°¨æ…æ¨èï¼šæ¨¡å‹è®¤ä¸ºè¿™å¯èƒ½æ˜¯ä¸€ä¸ªä¸é”™çš„æœºä¼š")
        else:
            explanation_parts.append("ğŸ”´ ä¸æ¨èï¼šæ¨¡å‹è®¤ä¸ºé£é™©è¾ƒé«˜")
        
        # ä¸»è¦é©±åŠ¨å› ç´ 
        if shap_values:
            sorted_shap = sorted(
                shap_values.items(),
                key=lambda x: abs(x[1]),
                reverse=True
            )[:3]
            
            positive_factors = []
            negative_factors = []
            
            for feature, value in sorted_shap:
                if value > 0:
                    positive_factors.append(self._translate_feature_name(feature))
                else:
                    negative_factors.append(self._translate_feature_name(feature))
            
            if positive_factors:
                explanation_parts.append(f"âœ… ç§¯æå› ç´ : {', '.join(positive_factors)}")
            
            if negative_factors:
                explanation_parts.append(f"âš ï¸ é£é™©å› ç´ : {', '.join(negative_factors)}")
        
        # ä¿¡å·ç±»å‹ç‰¹å®šè§£é‡Š
        signal_type = opportunity.get('signal_type', '')
        if signal_type == 'arbitrage':
            profit = opportunity.get('data', {}).get('profit_pct', 0)
            explanation_parts.append(f"ğŸ’° å¥—åˆ©åˆ©æ¶¦: {profit:.2f}%")
        elif signal_type == 'sentiment_shift':
            sentiment_delta = opportunity.get('data', {}).get('sentiment_delta', 0)
            explanation_parts.append(f"ğŸ˜Š æƒ…ç»ªå˜åŒ–: {sentiment_delta:+.2f}")
        
        return " | ".join(explanation_parts)
    
    def _translate_feature_name(self, feature: str) -> str:
        """å°†æŠ€æœ¯ç‰¹å¾åè½¬æ¢ä¸ºæ˜“æ‡‚çš„æè¿°"""
        translations = {
            'price': 'ä»·æ ¼èµ°åŠ¿',
            'volume': 'æˆäº¤é‡',
            'sentiment_score': 'å¸‚åœºæƒ…ç»ª',
            'dev_activity_score': 'å¼€å‘æ´»è·ƒåº¦',
            'whale_movement_count': 'å·¨é²¸æ´»åŠ¨',
            'rsi': 'RSIæŒ‡æ ‡',
            'macd': 'MACDæŒ‡æ ‡',
            'spread': 'ä¹°å–ä»·å·®',
            'gas_price': 'Gasè´¹ç”¨',
            'mention_count': 'ç¤¾äº¤æåŠé‡'
        }
        
        for key, value in translations.items():
            if key in feature.lower():
                return value
        
        return feature
    
    def _calculate_model_confidence(self, prediction: float) -> float:
        """
        è®¡ç®—æ¨¡å‹ç½®ä¿¡åº¦
        æç«¯é¢„æµ‹å€¼ï¼ˆæ¥è¿‘0æˆ–1ï¼‰è¡¨ç¤ºé«˜ç½®ä¿¡åº¦
        """
        distance_from_middle = abs(prediction - 0.5)
        confidence = distance_from_middle * 2
        return min(confidence, 1.0)
    
    async def _extract_features(self, opportunity: Dict) -> Optional[Dict[str, np.ndarray]]:
        """
        æå–æ··åˆæ¨¡å‹æ‰€éœ€çš„ç‰¹å¾
        """
        try:
            # è¿™é‡Œåº”è¯¥ä»æ•°æ®åº“è·å–å†å²æ•°æ®
            # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬ç”Ÿæˆç¤ºä¾‹æ•°æ®
            
            # æ—¶åºç‰¹å¾ï¼ˆLSTMè¾“å…¥ï¼‰
            sequence_features = np.random.randn(
                1, 
                self.sequence_length,
                len(self.time_series_features)
            )
            
            # æ¨ªæˆªé¢ç‰¹å¾ï¼ˆå½“å‰æ—¶åˆ»çš„ç‰¹å¾ï¼‰
            cross_features = np.random.randn(
                1,
                len(self.cross_sectional_features)
            )
            
            return {
                'sequence': sequence_features,
                'cross_sectional': cross_features
            }
            
        except Exception as e:
            logger.error(f"ç‰¹å¾æå–å¤±è´¥: {e}")
            return None
    
    def _get_all_feature_names(self) -> List[str]:
        """è·å–æ‰€æœ‰ç‰¹å¾åç§°"""
        # æ—¶åºç‰¹å¾å±•å¹³
        sequence_names = []
        for i in range(self.sequence_length):
            for feature in self.time_series_features:
                sequence_names.append(f"{feature}_t-{i}")
        
        # åŠ ä¸Šæ¨ªæˆªé¢ç‰¹å¾
        return sequence_names + self.cross_sectional_features
    
    async def train_hybrid_model(
        self,
        training_data: pd.DataFrame
    ):
        """
        è®­ç»ƒæ··åˆæ¨¡å‹
        """
        logger.info("å¼€å§‹è®­ç»ƒLSTM+XGBoostæ··åˆæ¨¡å‹...")
        
        try:
            # å‡†å¤‡æ•°æ®
            X_sequence, X_cross, y = await self._prepare_training_data(training_data)
            
            # æ—¶é—´åºåˆ—åˆ†å‰²ï¼ˆä¸èƒ½ç”¨éšæœºåˆ†å‰²ï¼‰
            tscv = TimeSeriesSplit(n_splits=5)
            
            best_score = 0
            best_model = None
            
            for train_idx, val_idx in tscv.split(X_sequence):
                X_seq_train, X_seq_val = X_sequence[train_idx], X_sequence[val_idx]
                X_cross_train, X_cross_val = X_cross[train_idx], X_cross[val_idx]
                y_train, y_val = y[train_idx], y[val_idx]
                
                # è®­ç»ƒæ··åˆæ¨¡å‹
                model = self._build_hybrid_model(
                    sequence_shape=(self.sequence_length, len(self.time_series_features)),
                    cross_sectional_shape=(len(self.cross_sectional_features),)
                )
                
                # æ—©åœå’Œå­¦ä¹ ç‡è°ƒæ•´
                callbacks = [
                    EarlyStopping(
                        monitor='val_loss',
                        patience=10,
                        restore_best_weights=True
                    ),
                    ReduceLROnPlateau(
                        monitor='val_loss',
                        factor=0.5,
                        patience=5,
                        min_lr=0.00001
                    )
                ]
                
                history = model.fit(
                    [X_seq_train, X_cross_train],
                    y_train,
                    validation_data=([X_seq_val, X_cross_val], y_val),
                    epochs=50,
                    batch_size=32,
                    callbacks=callbacks,
                    verbose=0
                )
                
                # è¯„ä¼°
                val_score = model.evaluate(
                    [X_seq_val, X_cross_val],
                    y_val,
                    verbose=0
                )[1]  # accuracy
                
                if val_score > best_score:
                    best_score = val_score
                    best_model = model
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if best_model:
                best_model.save(self.model_path / 'hybrid_model.h5')
                self.models['hybrid_model'] = best_model
                logger.info(f"âœ… æ··åˆæ¨¡å‹è®­ç»ƒå®Œæˆï¼Œæœ€ä½³å‡†ç¡®ç‡: {best_score:.3f}")
            
            # åŒæ—¶è®­ç»ƒXGBoostç”¨äºSHAPè§£é‡Š
            # åˆå¹¶ç‰¹å¾
            X_all = np.concatenate([
                X_sequence.reshape(X_sequence.shape[0], -1),
                X_cross
            ], axis=1)
            
            X_train, X_val, y_train, y_val = train_test_split(
                X_all, y, test_size=0.2, random_state=42
            )
            
            xgb_model = self._train_xgboost_model(X_train, y_train, X_val, y_val)
            
            # ä¿å­˜XGBoostæ¨¡å‹
            joblib.dump(xgb_model, self.model_path / 'xgboost_model.pkl')
            self.models['xgboost_model'] = xgb_model
            
            # é‡æ–°åˆå§‹åŒ–SHAPè§£é‡Šå™¨
            self._initialize_shap_explainers()
            
        except Exception as e:
            logger.error(f"è®­ç»ƒæ··åˆæ¨¡å‹å¤±è´¥: {e}", exc_info=True)
    
    async def _prepare_training_data(
        self,
        data: pd.DataFrame
    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
        å‡†å¤‡è®­ç»ƒæ•°æ®
        """
        # è¿™é‡Œåº”è¯¥å®ç°çœŸå®çš„æ•°æ®å‡†å¤‡é€»è¾‘
        # ä¸ºäº†æ¼”ç¤ºï¼Œè¿”å›ç¤ºä¾‹æ•°æ®
        n_samples = 1000
        
        X_sequence = np.random.randn(
            n_samples,
            self.sequence_length,
            len(self.time_series_features)
        )
        
        X_cross = np.random.randn(
            n_samples,
            len(self.cross_sectional_features)
        )
        
        y = np.random.randint(0, 2, n_samples)
        
        return X_sequence, X_cross, y
    
    async def _load_models(self):
        """åŠ è½½å·²ä¿å­˜çš„æ¨¡å‹"""
        try:
            # åŠ è½½æ··åˆæ¨¡å‹
            hybrid_path = self.model_path / 'hybrid_model.h5'
            if hybrid_path.exists():
                self.models['hybrid_model'] = tf.keras.models.load_model(hybrid_path)
                logger.info("âœ… åŠ è½½æ··åˆæ¨¡å‹")
            
            # åŠ è½½XGBoostæ¨¡å‹
            xgb_path = self.model_path / 'xgboost_model.pkl'
            if xgb_path.exists():
                self.models['xgboost_model'] = joblib.load(xgb_path)
                logger.info("âœ… åŠ è½½XGBoostæ¨¡å‹")
            
            # åŠ è½½æ ‡å‡†åŒ–å™¨
            for scaler_type in ['sequence_scaler', 'cross_scaler']:
                scaler_path = self.model_path / f'{scaler_type}.pkl'
                if scaler_path.exists():
                    self.scalers[scaler_type] = joblib.load(scaler_path)
            
        except Exception as e:
            logger.error(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
    
    async def _periodic_retrain(self):
        """å®šæœŸé‡è®­ç»ƒæ¨¡å‹"""
        while True:
            try:
                # ç­‰å¾…é…ç½®çš„æ—¶é—´é—´éš”
                await asyncio.sleep(86400)  # æ¯å¤©é‡è®­ç»ƒ
                
                logger.info("å¼€å§‹å®šæœŸæ¨¡å‹é‡è®­ç»ƒ...")
                
                # è·å–æœ€æ–°è®­ç»ƒæ•°æ®
                # training_data = await self._fetch_training_data()
                
                # if training_data is not None and len(training_data) > 1000:
                #     await self.train_hybrid_model(training_data)
                
            except Exception as e:
                logger.error(f"å®šæœŸé‡è®­ç»ƒå¤±è´¥: {e}")